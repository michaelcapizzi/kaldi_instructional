{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1: Building a language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need some background on `n-gram` language modeling, Stanford has a very good set of slides explaining it.  They can be found in `resource_files/resources/language_modeling.pdf`.\n",
    "\n",
    "`kaldi` has native support for the `ARPA` format for language models.  A good explanation of that format can be read [here](https://cmusphinx.github.io/wiki/arpaformat/).\n",
    "\n",
    "A popular open-source language modeling toolkit that outputs in the `ARPA` format is `IRSTLM`.  It's manual can be found in `resource_files/resources/irstlm-manual.pdf`.\n",
    "\n",
    "We will build language models from a toy corpus (using `IRSTLM`) and then examine it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the toy corpus\n",
    "\n",
    "A toy corpus is in `resource_files/language_model/animal_corpus.txt`.  In this corpus, each line represents a sentence, and there is *no* punctuation present.\n",
    "\n",
    "**Note:** From the perspective of a language model, one *could* model punctuation if that were of importance, but since our purpose is to model *spoken* text, we do *not* have any need to model punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat resource_files/language_model/animal_corpus.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## building the language model with `IRSTLM`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After `export`ing a few variables, we will be able to call scripts from `IRSTLM` without a full path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export IRSTLM=${KALDI_PATH}/tools/irstlm\n",
    "export PATH=${PATH}:${IRSTLM}/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `add-start-end.sh`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our corpus does *not* have periods, we need to add a custom symbol to represent the *beginning* and *end* of each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add-start-end.sh -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat resource_files/language_model/animal_corpus.txt | add-start-end.sh > resource_files/language_model/animal_corpus_start_stop.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat resource_files/language_model/animal_corpus_start_stop.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `build-lm.sh`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the actual language model using `build-lm.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build-lm.sh -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main arguments we will focus on are:\n",
    " - `-i`\n",
    " - `-o`\n",
    " - `-n`\n",
    "\n",
    "`-k` is an important argument for efficient language modeling on a very large corpus.  With our toy example, we do not need to worry about that.  You'll also notice a number of options for `-s` which relate to the type of `smoothing` used.  Stanford has a great resource on `smoothing` that you can find in `resource_files/smoothing_explained.pdf`.  For now, we will ignore both of these arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build-lm.sh \\\n",
    "    -i resource_files/language_model/animal_corpus_start_stop.txt \\\n",
    "    -o resource_files/language_model/animal_lm-2_gram.iarpa \\\n",
    "    -n 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`IRSTLM` automatically `compresses` the resulting language model.  So we will `decompress` it so we can look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gzip -d resource_files/language_model/animal_lm-2_gram.iarpa.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat resource_files/language_model/animal_lm-2_gram.iarpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `iARPA` to `ARPA` format\n",
    "\n",
    "You'll notice the header line of our language model above says `iARPA`.  The `IRSTLM` manual explains:\n",
    "\n",
    "```\n",
    "Notice that build-lm.sh produces a LM file train.ilm.gz that is NOT in the final ARPA format, but in an intermediate format called iARPA, that is recognized by the compile-lm command and by the Moses SMT decoder running with IRSTLM. \n",
    "```\n",
    "\n",
    "It explains the different between `iARPA`:\n",
    "\n",
    "```\n",
    "This is an intermediate ARPA format in the sense that each entry of the file does not contain in the first position the full n-gram probability, but just its smoothed frequency.\n",
    "```\n",
    "\n",
    "And so we must run a final step over our language model (using `compile-lm`) in order to create the proper `ARPA` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compile-lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compile-lm resource_files/language_model/animal_lm-2_gram.iarpa --text=yes resource_files/language_model/animal_lm-2_gram.arpa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice some small differences in the values of `ARPA` compared to `iARPA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff resource_files/language_model/animal_lm-2_gram.arpa resource_files/language_model/animal_lm-2_gram.iarpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build a `2-gram` language model that does **not** include `start` and `stop` symbols.  We can do this by using our original `animal_corpus.txt` file as `input`.\n",
    "\n",
    "**Note:** We can run `compile-lm` over the `gz`ed output of `build-lm.sh`, so we can skip the manual step of `decompress`ing the `iarpa.gz` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build-lm.sh \\\n",
    "    -i resource_files/language_model/animal_corpus.txt \\\n",
    "    -o resource_files/language_model/animal_lm-2_gram-no_start_stop.iarpa \\\n",
    "    -n 2\n",
    "    \n",
    "compile-lm \\\n",
    "    resource_files/language_model/animal_lm-2_gram-no_start_stop.iarpa.gz \\\n",
    "    --text=yes \\\n",
    "    resource_files/language_model/animal_lm-2_gram-no_start_stop.arpa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat resource_files/language_model/animal_lm-2_gram-no_start_stop.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also build a `3-gram` and a `4-gram` model, both using `start` and `stop` symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in 3 4; do\n",
    "    lm_out=resource_files/language_model/animal_lm-${n}_gram\n",
    "    \n",
    "    # build the `iarpa` format\n",
    "    build-lm.sh \\\n",
    "        -i resource_files/language_model/animal_corpus_start_stop.txt \\\n",
    "        -o ${lm_out}.iarpa \\\n",
    "        -n ${n}\n",
    "\n",
    "    # compile into `arpa` format\n",
    "    compile-lm \\\n",
    "        ${lm_out}.iarpa.gz \\\n",
    "        --text=yes \\\n",
    "        ${lm_out}.arpa\n",
    "\n",
    "    # decompress `iarpa` format\n",
    "    gzip -d ${lm_out}.iarpa.gz\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll just `decompress` the remaining `gzip`ped file, and then we should have **four** language models, in both the `ARPA` and the `iARPA` formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gzip -d resource_files/language_model/*.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls resource_files/language_model | grep \"\\.iarpa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls resource_files/language_model | grep \"\\.arpa\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using `ARPA` formats in our `ASR` pipeline, however, the `python` package we will use in the next notebook to examine the language models requires the `iARPA` format.\n",
    "\n",
    "Unfortunately, that `python` package is a bit picky about formatting, and so we have to run a quick `sed` command over our `.iarpa` language models to make them acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lm in `ls resource_files/language_model/*.iarpa`; do\n",
    "    sed -i.bak -E \"s:([\\-\\.0-9]+) :\\1\\t:g\" ${lm}\n",
    "    rm resource_files/language_model/*.bak\n",
    "done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
