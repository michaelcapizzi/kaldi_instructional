{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2: Examining the *required* files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our version of the `kaldi` pipeline will depend on the files and directory structures explained in this notebook.\n",
    "\n",
    "**Note**: If you would like to run this pipeline with your **own data**, you **must** have all of the following before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, all of the things we need should now be in the directory, `raw_data`.  Here you will see the original `.tar.gz` downloads along with:\n",
    "\n",
    " - the `lexicon` file: `librispeech-lexicon.txt`\n",
    " - the `transcripts` file: `librispeech-transcripts.txt`\n",
    " - the different `language models`: `3-gram.arpa.gz`, `3-gram.pruned.*.arpa.gz`, `4-gram.arpa.gz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional files for the different subsets of the dataset are in `raw_data/LibriSpeech`, including an `audio` file and a `data` file for each subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls raw_data/LibriSpeech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## audio files\n",
    "\n",
    "All audio files for a particualr subset (*e.g.* `train`, `dev`, `test`, etc.) must be in a flat directory structure (*i.e.* without any sub-directories).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls raw_data/LibriSpeech/test-clean_audio | head    # look at first 10 files in the directory to exhibit flat structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the format of the audio is important as well.  `kaldi` expects the audio to be encoded with `16-bit signed little endian` (more information about this is [here](https://wiki.multimedia.cx/index.php/PCM)).  \n",
    "\n",
    "The sample rate of that audio is a hyperparameter that becomes important in a later step.  Most common is `16 kHz` for recorded audio and `8 kHz` for recorded phone calls.  In our case, we downsampled the `librispeech` data to also be `8 kHz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file raw_data/LibriSpeech/test-clean_audio/1089-134686-0000.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command below will generate counts for each subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for part in train-clean-100_audio dev-clean_audio dev-other_audio test-clean_audio test-other_audio; do\n",
    "    count=$(ls raw_data/LibriSpeech/${part} | wc -l)\n",
    "    echo \"There are ${count} utterances in the ${part} subset\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## segments file \n",
    "#### (**UNUSED** for `librispeech`)\n",
    "\n",
    "The `librispeech` data is already segmented into small audio files (~2-10 seconds long).  `kaldi` *can* handle data that unsegmented, but it requires an additional `segments` file with the following format:\n",
    "\n",
    "```\n",
    "[utterance-id] [audio-basename] [utterance-start] [utterance-stop]\n",
    "[utterance-id] [audio-basename] [utterance-start] [utterance-stop]\n",
    "[utterance-id] [audio-basename] [utterance-start] [utterance-stop]\n",
    "```\n",
    "\n",
    "This allows `kaldi` to process **each segment** as a separate audio file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transcript file\n",
    "\n",
    "All transcripts for all audio subsets should be in one text file with the following format:\n",
    "```\n",
    "[utterance-id] [transcript text]\n",
    "[utterance-id] [transcript text]\n",
    "[utterance-id] [transcript text]\n",
    "```\n",
    "\n",
    "The `utterance-id` is used to identify the particular utterance.  In the case of **segmented** audio like the `librispeech` dataset, this will **also** be the audio basename (*i.e.* without `.wav`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head raw_data/librispeech-transcripts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## phones file\n",
    "\n",
    "This file contains a list of all the phones used to make up the words in our `lexicon`.\n",
    "\n",
    "It takes the following format:\n",
    "\n",
    "```\n",
    "[phone_1]\n",
    "[phone_2]\n",
    "[phone_3]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head raw_data/librispeech-phones.txt\n",
    "tail raw_data/librispeech-phones.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that some phones have a digit at their end.  This allows for us to distinguish different stresses or tones of a phone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to identify `silence phones`.  These are phones that will represent sounds of things that do **not** correspond to spoken words.  This could be laughter, coughing, passing cars, whatever.  In our case, we will keep it simple and have one `phone` (`SIL`) to represent everything non-spoken.  But you can be as granular with this as you'd like **as long as** your `transcripts` file accurately transcribes them all!\n",
    "\n",
    "**Note:** This `silence phone` will be a hyperparameter to the first step in our pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat raw_data/librispeech-phones.txt | grep -A2 -B2 SIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lexicon\n",
    "\n",
    "The `lexicon` is a file containing all the words in our vocabulary **and** their pronunciations.  \n",
    "\n",
    "**Note:** Only words that appear in this `lexicon` will be words that our `ASR` system predicts.  In other words, if the word is **not** in this `lexicon`, then our system will **never** be able to predict it.\n",
    "\n",
    "It takes the following format:\n",
    "\n",
    "```\n",
    "[short_word]    [phone_1] [phone_2] [phone_3]\n",
    "[longer_word]   [phone_1] [phone_2] [phone_3] [phone_4] [phone_5]\n",
    "[another_word]  [phone_1] [phone_2] [phone_3]\n",
    "```\n",
    "\n",
    "**Note:** The first `whitespace` is a `tab`, the remaining are `space`.\n",
    "**Note:** While it is convenient for humans, `kaldi` does **not** require that this file be in alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head -n5 raw_data/librispeech-lexicon.txt\n",
    "tail -n5 raw_data/librispeech-lexicon.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also notice multiple entries for the same word are allowed, provided they have different pronunciations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat raw_data/librispeech-lexicon.txt | grep \"INDIRECTLY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have an entry for unknown words (`<unk>`).  `kaldi` requires this \"placeholder\" for any words that it can't decode.  It is made up of the single `nonsilence phone`, `SIL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat raw_data/librispeech-lexicon.txt | grep \"<unk>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## language model\n",
    "\n",
    "The `language model` must be in the `ARPA` format (more details on this format in week 2).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head raw_data/3-gram.pruned.3e-7.arpa\n",
    "echo \"...\"\n",
    "grep -A4 -E '\\\\2-grams' raw_data/3-gram.pruned.3e-7.arpa\n",
    "echo \"...\"\n",
    "grep -A4 -E '\\\\3-grams' raw_data/3-gram.pruned.3e-7.arpa\n",
    "echo \"...\"\n",
    "tail raw_data/3-gram.pruned.3e-7.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The `language model` **can** remain `compress`ed, but it requires `piping` `gzip -d` to later steps.  We have already uncompressed the `language model`s for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 4 `language model`s of different size and complexity built for the `librispeech` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls raw_data | grep \"arpa\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
