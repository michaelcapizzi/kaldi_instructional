{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2: Examining language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ARPA` (and `iARPA`) format is very interpretable.  If you haven't yet done so, read this short [blog post](https://cmusphinx.github.io/wiki/arpaformat/) for more information on how to interpret them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat resource_files/language_model/animal_lm-2_gram.iarpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `PyNLPl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the [`PyNLPl`](http://pynlpl.readthedocs.io/en/latest/) (pronounced \"pineapple\") package in `python` to examine our language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynlpl.lm.lm as lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading in `.iARPA` files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ARPALanguageModel()` will import an existing **`iARPA`** formatted language model.\n",
    "\n",
    "**Note:** Recall that in the last notebook we had to run a quick `sed` command over the `.iarpa` format because there were times where the whitespace between a probability and the `1-gram` was a `\" \"` instead of a `\\t`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_gram_lm = lm.ARPALanguageModel(\n",
    "    filename=\"resource_files/language_model/animal_lm-2_gram.iarpa\",\n",
    "    base_e=False,  # this will keep the log probabilities in `base 10` so that they match up with the original file\n",
    "    debug=True     # this argument will allow you to more easily see how the data is stored in the object\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that each `n-gram` is stored as a `<tuple>`, even `1-gram`s ==> `([word],)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### looking up **existing** `n-gram`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.ngrams` contains all the of `n-gram`s **present** in our language model.  We can access either:\n",
    " - the probability ==> `.prob()`\n",
    " - the backoff probability ==> `.backoff()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_gram_lm.ngrams.prob((\"dog\",)), bi_gram_lm.ngrams.backoff((\"dog\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm this by double-checking the values in the original `.iarpa` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat resource_files/language_model/animal_lm-2_gram.iarpa | grep -P \"\\tdog\\t\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_gram_lm.ngrams.prob((\"the\", \"dog\")), bi_gram_lm.ngrams.backoff((\"the\", \"dog\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat resource_files/language_model/animal_lm-2_gram.iarpa | grep -P \"the dog\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we try to lookup an `n-gram` that does **not** exist in the language model explicitly, we get a `KeyError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    bi_gram_lm.ngrams.prob((\"human\", \"ate\"))\n",
    "except Exception as e:\n",
    "    print(\"n-gram {} doesn't exist in language model\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    bi_gram_lm.ngrams.prob((\"the\", \"dog\", \"ate\"))\n",
    "except Exception as e:\n",
    "    print(\"n-gram {} doesn't exist in language model\".format(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculating new `n-gram` probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two cases where this will occur:\n",
    " 1. The `n-gram` is of the size of the language model **but** this particular `n-gram` is **not found** in the language model.\n",
    " 2. The `n-gram` is **larger** than that of the language model.  In other words, you want the probability of a `3-gram`, but your language model is only made up of `2-gram`s.\n",
    " \n",
    "In both cases, we can use `.score()`.  To score a new `n-gram`, provide that `n-gram` as a `<tuple>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `n-gram` is **not present** in language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In cases like these, we need to access `backoff` probabilities, which are designed precisely for this purpose.  You'll notice that in our `2-gram` language model, backoff probabilities exist for `1-gram`s only.  It is the number that comes **after** the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat resource_files/language_model/animal_lm-2_gram.iarpa | grep -A15 -E \"1-grams\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we wanted to find the probability of \"human ate\", it would be calculated as:\n",
    "\n",
    "$p(human\\_ate) = p(human) + p(ate|human) = p(human) + p(UNK) + bWt(human)$\n",
    "\n",
    "**Note:** Remember because our probabilities are in `negative log`-space, we will **add** instead of **multiply**.  And since all of our probabilities will be `negative`, the **closer** the probability is to `0`, the \"more likely\" the word/sequence is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_gram_lm.score((\"human\", \"ate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm this by doing the calculations ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_gram_lm.ngrams.prob((\"human\",)) + \\\n",
    "bi_gram_lm.ngrams.prob((\"<unk>\",)) + \\\n",
    "bi_gram_lm.ngrams.backoff((\"human\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If you forget to enter the `n-gram` as a `<tuple>`, the `<string>` will be considered an `n-gram` of **characters**, **none of which** will be present in the language model, so it will be equal to $p(UNK) * len(string)$\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_gram_lm.score(\"human ate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = 0\n",
    "for i in \"human ate\":\n",
    "    result += bi_gram_lm.scoreword(i)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `n-gram` is **larger** than language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to get the probability of the sequence, `\"the dog ate\"` using our `2-gram` language model, it will be calculated as follows:\n",
    "\n",
    "$p(the\\_dog\\_ate) = p(the) + p(dog|the) + p(ate|dog)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_gram_lm.score((\"the\", \"dog\", \"ate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can again confirm this by doing the calculation ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_gram_lm.ngrams.prob((\"the\",)) + \\\n",
    "bi_gram_lm.ngrams.prob((\"the\", \"dog\")) + \\\n",
    "bi_gram_lm.ngrams.prob((\"dog\", \"ate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if any one of the `n-grams` we use to compose our sequence is **not** present in our language model, we again need to utilize the `backoff` probabilities.\n",
    "\n",
    "$p(the\\_triceratops\\_ate) = p(the) + p(triceratops|the) + p(ate|triceratops) = p(the) + p(UNK) + bWt(the) + p(ate) + bWt(triceratops)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_gram_lm.score((\"the\", \"triceratops\", \"ate\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_gram_lm.ngrams.prob((\"the\",)) + \\\n",
    "bi_gram_lm.ngrams.prob((\"<unk>\",)) + \\\n",
    "bi_gram_lm.ngrams.backoff((\"the\",)) + \\\n",
    "bi_gram_lm.ngrams.prob((\"ate\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing probabilities\n",
    "\n",
    "Now that we have the tools, let's look at how \"likely\" particular sequences of words will be given our language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our animal corpus provided us with a \"hierarchical\" understanding of the food chain.  For example, our `\"mouse\"` could only eat **one** thing while our `\"lion\"` ate **four** things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat resource_files/language_model/animal_corpus.txt | grep \"mouse ate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat resource_files/language_model/animal_corpus.txt | grep \"lion ate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so intuitively, the `2-gram` `\"mouse ate\"` should be **four times less likely** than `\"lion ate\"`.\n",
    "\n",
    "**Note:** Remember that these probabilities are in `log base 10` space, so we need to do a quick conversion in order to see the expected ratio between the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10**bi_gram_lm.ngrams.prob((\"mouse\", \"ate\")), 10**bi_gram_lm.ngrams.prob((\"lion\", \"ate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as the `\"lion\"` is relatively \"high up\" in our food chain, it is eaten by less things than the `\"mouse\"` is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat resource_files/language_model/animal_corpus.txt | grep \"ate the lion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat resource_files/language_model/animal_corpus.txt | grep \"ate the mouse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we should expect the probability of `\"ate the lion\"` to be **three times less likely** than `\"ate the mouse`\".  And since we are dealing with a `2-gram` language model, we will need to do some probability calculations this time (instead of being able to simply \"look up\" the probabilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10**bi_gram_lm.score((\"ate\", \"the\", \"lion\")), 10**bi_gram_lm.score((\"ate\", \"the\", \"mouse\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this is clearly not the case!  In fact, `\"ate the lion\"` is **more likely** than `\"ate the mouse`\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remembering how these probabilities are calculated, we can see why this is the case:\n",
    "    \n",
    "$p(ate\\_the\\_XXX) = p(ate) + p(ate|the) + p(XXX|the)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_gram_lm.ngrams.prob((\"the\", \"lion\")), bi_gram_lm.ngrams.prob((\"the\", \"mouse\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the `\"lion\"` appeared **one time more than** `\"mouse\"` (notice that the `\"t-rex\"` did **not** eat the `\"mouse\"`), this increased probability impacted our `3-gram` calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But then, in that case, our `3-gram` language model should be able handle this problem better.  It would have modeled `\"ate the XXX\"` explicitly and would not, therefore, need to generate a probability by considering the smaller `n-gram`s that caused us problems above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_gram_lm = lm.ARPALanguageModel(\n",
    "    filename=\"resource_files/language_model/animal_lm-3_gram.iarpa\",\n",
    "    base_e=False  # this will keep the log probabilities in `base 10` so that they match up with the original file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so in this case, using `score()` will simply require looking up (`.ngrams.prob()`) the explicit `3-gram` that was captured by the language model explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10**tri_gram_lm.score((\"ate\", \"the\", \"lion\")), 10**tri_gram_lm.score((\"ate\", \"the\", \"mouse\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10**tri_gram_lm.ngrams.prob((\"ate\", \"the\", \"lion\")), 10**tri_gram_lm.ngrams.prob((\"ate\", \"the\", \"mouse\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, our intuitions match.  `\"ate the lion\"` is **three times less likely** than `\"ate the mouse\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, the higher the order of `n-gram` you are willing to model, the more \"accurate\" your language modeling will become.  But at a certain point, this becomes unwieldly.  You can see that our `librispeech` data comes with a `3-gram` **and** a `4-gram` model, but notice the size doubles just going from `3-gram`s to `4-gram`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls -lah raw_data/ | grep -E \"gram\\.arpa\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And by looking at the number of `n-gram`s modeled, this shouldn't be a surprise as there are ~60 million `4-gram`s in the `4-gram` language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head -n5 raw_data/3-gram.arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head -n6 raw_data/4-gram.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pruning` a language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where `pruning` can come in handy.  Since a good number of the total `n-gram`s that appear in a corpus are infrequent (remember, any `n-gram` seen **even once** has to be modeled), these can be removed from the language model without reducing the \"accuracy\" of the model too much.\n",
    "\n",
    "The `IRSTLM` manual (found in `resource_files/resources/irstlm-manual.pdf`) explains pruning this way:\n",
    "\n",
    "```\n",
    "Large LMs files can be pruned in a smart way by means of the command prune-lm that removes n-grams for which resorting to the back-off results in a small loss.\n",
    "```\n",
    "\n",
    "The `librispeech` data provides two `pruned` `3-gram` language models, each with a different `pruning` threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls -lh raw_data/ | grep 3-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the significant reduction in size.  This will be evident in the number of `2-gram`s and `3-gram`s as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head -n5 raw_data/3-gram.arpa\n",
    "echo ...\n",
    "head -n5 raw_data/3-gram.pruned.1e-7.arpa\n",
    "echo ...\n",
    "head -n5 raw_data/3-gram.pruned.3e-7.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it's not necessary to `prune` our toy animal language models, it **is** easy to do with `IRSTLM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export IRSTLM=${KALDI_PATH}/tools/irstlm\n",
    "export PATH=${PATH}:${IRSTLM}/bin\n",
    "prune-lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You can provide a different threshold for each `n-gram`.  In the `librispeech` language models, you can see that no `pruning` was done on `1-gram`s."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
