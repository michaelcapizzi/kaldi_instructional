{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.2: Examining `G.fst` with `openFST` \n",
    "## (in `python` with `pywrapfst`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`openFST` has a `python` wrapper called [`pywrapfst`](http://www.openfst.org/twiki/bin/view/FST/PythonExtension) that gives us *most* of the functionality of `openFST` but from inside `python`.\n",
    "\n",
    "We will also use some custom methods that are in `utils/fst_manipulate/fst_manipulate.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because of the way `kaldi` installed `openFST` we have to add the path to the python functions here\n",
    "import sys\n",
    "sys.path.append(\"/scratch/kaldi/tools/openfst-1.6.2/lib/python2.7/site-packages\")    \n",
    "\n",
    "from utils.fst_manipulate import fst_manipulate as fstman  # scripts to further manipulate fsts\n",
    "\n",
    "import pywrapfst as openfst  # the wrapper module\n",
    "import graphviz as dot       # a wrapper for graphviz, which will allow us to visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read in the `fst`\n",
    "\n",
    "Using `Fst.read()` we can load the `fst` into an easy-to-read format with the following structure:\n",
    "\n",
    " ```\n",
    " from_state     to_state    arc_symbol    weight(-log)\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_in = openfst.Fst.read(\"resource_files/fst/animal_fst-2_gram.fst\")\n",
    "print(fst_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By simply calling the variable `fst_in`, this notebook will automatically render the `FST` for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write to `.dot`\n",
    "\n",
    "We can also write this `fst` into a `.dot` (from `graphviz`) format (in case we want to visualize it outside of this noteboook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_in.draw(\"resource_files/fst/animal_fst-2_gram.dot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the default setting is to write the `.dot` so that the image is in `landscape` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head resource_files/fst/animal_fst-2_gram.dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't visualize well in these notebooks, so the `python` method below will *wrap* both the `FST.draw()` command along with an in-place edit of the `.dot` file to `orientation = Portrait`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fstman.write_wrapper(\n",
    "    fst_=fst_in, \n",
    "    path_out=\"resource_files/fst/animal_fst-2_gram.dot\"\n",
    ")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may prefer to render the `.dot` format in your browser using a site like http://www.webgraphviz.com/.  Just copy the `.dot` text from `resource_files/fst/animal_fst-2_gram.dot`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analyzing the `fst`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This [blog post](http://vpanayotov.blogspot.com/2012/06/kaldi-decoding-graph-construction.html), which we'll revisit next week, has a section about converting our language model to an `FST`.  The relevant image and portion are below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sample_g](resource_files/fst/sample_G.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```One thing to keep in mind here is that the weights of the FSTs are calculated by negating the natural logarithm of the probabilities, and the quantities given in ARPA file format are base 10 logarithms of probabilities. The WFST produced by arpa2fst is really straightforward but let's look little closer. There is a start node representing the start of an utterance (node 0), separate nodes for each of our \"ache\", \"Cay\", \"K.\" vocabulary words (nodes 6, 4 and 5 respectively), a back-off node (1) and a final node(2) for the end of an utterance. Let's for the sake of the example trace a path through the graph corresponding to a bigram - say \"<s> ache\" bigram. Because there is no such bigram in our toy corpus we are forced to take the route through the back-off node, i.e. the arcs 0-3, 3-1, 1-6. The weight of the first arc is 0 (i.e. 1 when converted to probability), the weight of 3-1 arc is 0.69315, which corresponds to the back-off probability for \"<s>\" (−ln(10−0.30103)), and the weight 2.0794 of 1-6 arc corresponds to the unigram probability of \"ache\" (−ln(10−0.9030899)).```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the format of any `FST`-form of a `2-gram` language model is this:\n",
    " - There is a `start` node\n",
    "    - The weight from `start` node to `<s>` will always be 0\n",
    " - There is a `node` for each word in vocabulary (including `<s>` and `</s>`)\n",
    "    - There will be an `arc` from each `node` that models a \"valid\" `2-gram`\n",
    "       - The weight will be `2-gram` probability of $p(to|from)$\n",
    "    - There will be an `arc` from the `backoff` node\n",
    "       - The weight will be the `unigram` probability $p(to)$\n",
    " - There is a `backoff` node\n",
    "    - There will be an `arc` from each `word-node`\n",
    "      - The `arc` `label` will be `<eps>`\n",
    "      - The weight will be the `backoff` `unigram` probability $p(from)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fst_manipulate.py` has a method called `index_fst()` which will parse an existing `FST` and return **TWO** `<dict>`s:\n",
    " - `<dict>` that will give you the `state_id` associated with every word along with (1) the `weight`s of any `arc` from another `word-state` and (2) the `weight`s of any `arc` to another `word-state`.\n",
    " - `<dict>` that is a lookup from `node_id` to `word`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_dict, node_2_word = fstman.index_fst(\n",
    "    fst_in=fst_in\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_2_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if I wanted to see what `node` represented the word `\"tyrannosaurus\"` in our `FST`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_dict[\"tyrannosaurus\"][\"state_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if I wanted to know what the probability of any **explicitly modeled** `2-gram` (like `\"the tyrannosaurus\"`)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_dict[\"tyrannosaurus\"][\"weights_from\"][\"the\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could also get the same value by trying this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_dict[\"the\"][\"weights_to\"][\"tyrannosaurus\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if you look closely at the `<dict>` for `weights_from` for the word `tyrannosaurus`, you'll notice it only has two entries: \n",
    " - `\"the\"`\n",
    " - `<eps>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_dict[\"tyrannosaurus\"][\"weights_from\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because the **only** explicitly modeled `2-gram` ending in `\"tyrannosarus\"` is `\"the tyrannosaurus\"`.  And remember, that `<eps>` is used to help us \"backoff\" for any unseen `2-gram`s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we tried to look up `\"cat tyrannosaurus\"`, I'll get a `KeyError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_dict[\"tyrannosaurus\"][\"weights_from\"][\"cat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_dict[\"cat\"][\"weights_to\"][\"tyrannousaurus\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead we'd need to look up `\"<eps> tyrannosaurus\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_dict[\"tyrannosaurus\"][\"weights_from\"][\"<eps>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_dict[\"<eps>\"][\"weights_to\"][\"tyrannosaurus\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use this `fst_dict` a little bit later.  But first, we need to look at two more things we can do with our `FST`:\n",
    "\n",
    " 1. checking to see if sequence if valid according to our language model\n",
    " 2. finding the shortest path for a given sequence and calculating its cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking to see if sequence is valid according to language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, we will use an `FST` to determine the most likely transcription of the audio sent as input to our `ASR` pipeline.  For now, we can use the `FST` representing our language model to see how we would need to travel through the `FST` in order to reprent any sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence = \"the rex ate the human\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fst_manipulate.py` has a method called `sequence_to_fst()` which will convert any sequence represented as a `<str>` into a \"mini\"-`FST`.\n",
    "\n",
    "It takes two arguments:\n",
    " - `seq_string` --> the `<str>` of the sequence we want to learn about\n",
    " - `lm_fst` --> the `FST` representing our language model; we need this to ensure we correctly map words to indices\n",
    " \n",
    "It will generate a very basic `FST` representing the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence_fst = fstman.sequence_to_fst(\n",
    "    seq_string=sample_sentence,    \n",
    "    lm_fst=fst_in                  \n",
    ")                                 \n",
    "sample_sentence_fst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then [`compose`](http://www.openfst.org/twiki/bin/view/FST/ComposeDoc) a new `FST` which is a combination of our original `FST` (representing our language model) and our \"mini\"-`FST` (representing our sequence we want to learn about).  \n",
    "\n",
    "If the sequence we provide **can** be modeled by our language model, then we should get out a resulting `FST`.  And if this `compose` step fails, we know that our language model is incapable of modeling that sequence.\n",
    "\n",
    "**Note:** Because we have a token representing `<unk>`, we will be able to model any sequence by our language model.\n",
    "\n",
    "`fst_manipulate.py` has a method called `check_sequence()` which does two things:\n",
    " 1. calls `sequence_to_fst()` to build the \"mini\"-`FST`\n",
    " 2. calls `compose` to build the composed `FST`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence_fst_out = fstman.check_sequence(\n",
    "    seq_string=sample_sentence, \n",
    "    lm_fst=fst_in\n",
    ")\n",
    "sample_sentence_fst_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see, essentially, just the portion of the language model that we need in order to model our sequence.  This means that our language model **can** model this sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finding the shortest path and calculating its cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But you'll also notice that there is **more than one** possible path through this `FST`.  So if we want to determine how **likely** this sentence is, we'll need to calculate the cost of the **shortest** path.  This cost will end up represting the **same** thing as the probability of the sequence according to our language model (`2.2 Examining language models`), but the numbers will **not** end up being the same.\n",
    "\n",
    "**Note:** This is because of `kaldi`-specific steps taken when converting the language model to an `FST`.  But the **relationship** between the cost of two sequences will still represent the same thing (*e.g.* if the cost of one sequence is **larger** than the cost of another sequence, then we know that the first sequence is **less likely**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fstmanipulate.py` has a method called `get_shortest_path()` that will return the shortest path through an `FST`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fstman.get_shortest_path(\n",
    "    fst_in=sample_sentence_fst_out\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to accumulate the cost for each arc to determine the overall cost of the path, which will represent the overall likelihood of the sentence according to our `FST`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, `fstmanipulate.py` has a method to do this: `calculate_cost()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fstman.calculate_cost(\n",
    "    fst_in=sample_sentence_fst_out\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, that this value is `negative log, base e` ($-ln$), so we need to do a quick conversion to get it out of `ln` space, and convert to a probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fst_manipulate.py` has a method called `convert_neg_log_e()` that will take $e^{-value}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fstman.convert_neg_log_e(\n",
    "    neg_log_e=fstman.calculate_cost(\n",
    "        fst_in=sample_sentence_fst_out\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing likelihoods\n",
    "\n",
    "If this `FST` is an accurate representation of our language model, then the comparisons that we made in `2.2 Examining language models` should still hold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `mouse ate` v. `lion ate`\n",
    "\n",
    "When we compared these two sequences before, we found that `\"lion ate\"` was **four times more likely** than `\"mouse ate\"`.  This shouldn't be surprising because, remember, we built our language model on the sentences below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat resource_files/language_model/animal_corpus.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the same relationship between these likelihoods is maintained in our `FST`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_ate = \"mouse ate\"\n",
    "mouse_ate_fst = fstman.check_sequence(        # generates the composed-FST of our sequence and the full FST\n",
    "    seq_string=mouse_ate,    \n",
    "    lm_fst=fst_in                  \n",
    ")\n",
    "mouse_ate_log_cost = fstman.calculate_cost(   # get the log cost\n",
    "    fst_in=mouse_ate_fst\n",
    ")      \n",
    "mouse_ate_cost = fstman.convert_neg_log_e(    # convert to probability\n",
    "    neg_log_e=mouse_ate_log_cost\n",
    ")  \n",
    "mouse_ate_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lion_ate = \"lion ate\"\n",
    "lion_ate_fst = fstman.check_sequence(         # generates the composed-FST of our sequence and the full FST\n",
    "    seq_string=lion_ate,    \n",
    "    lm_fst=fst_in                  \n",
    ")\n",
    "lion_ate_log_cost = fstman.calculate_cost(    # get the log cost\n",
    "    fst_in=lion_ate_fst\n",
    ")         \n",
    "lion_ate_cost = fstman.convert_neg_log_e(     # convert to probability\n",
    "    neg_log_e=lion_ate_log_cost\n",
    ")    \n",
    "lion_ate_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lion_ate_cost / mouse_ate_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the changes `kaldi` made during the conversion, this is close enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ate the mouse` v. `ate the lion`\n",
    "\n",
    "If you look back at `2.2 Examining language models`, you'll notice that our `2-gram` language model was *not* able to model the above sequences in such a way to match our intuitions.  \n",
    "\n",
    "We expected that, since three animals eat the `\"mouse\"`, but only one animal eats the `\"lion\"`, then `\"ate the mouse\"` should be **three times more likely** than `\"ate the lion\"`.  But this wasn't true.  This had to do with the fact that `\"the lion\"` appeared one more time than `\"the mouse\"` in our corpus.\n",
    "\n",
    "(Recall, we could fix this with a `3-gram` model that modeled `\"ate the mouse\"` and `\"ate the lion\"` as a single unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat resource_files/language_model/animal_corpus.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, the same relationship between the two sequences that we saw in the language model occur when we run them through the `FST`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ate_the_mouse = \"ate the mouse\"\n",
    "ate_the_mouse_fst = fstman.check_sequence(        # generates the composed-FST of our sequence and the full FST\n",
    "    seq_string=ate_the_mouse,    \n",
    "    lm_fst=fst_in                  \n",
    ")\n",
    "ate_the_mouse_log_cost = fstman.calculate_cost(   # get the log cost\n",
    "    fst_in=ate_the_mouse_fst\n",
    ")      \n",
    "ate_the_mouse_cost = fstman.convert_neg_log_e(    # convert to probability\n",
    "    neg_log_e=ate_the_mouse_log_cost\n",
    ")  \n",
    "ate_the_mouse_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ate_the_lion = \"ate the lion\"\n",
    "ate_the_lion_fst = fstman.check_sequence(         # generates the composed-FST of our sequence and the full FST\n",
    "    seq_string=ate_the_lion,    \n",
    "    lm_fst=fst_in                  \n",
    ")\n",
    "ate_the_lion_log_cost = fstman.calculate_cost(    # get the log cost\n",
    "    fst_in=ate_the_lion_fst\n",
    ")      \n",
    "ate_the_lion_cost = fstman.convert_neg_log_e(     # convert to probability\n",
    "    neg_log_e=ate_the_lion_log_cost\n",
    ")  \n",
    "ate_the_lion_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modifying the `FST`\n",
    "\n",
    "If we wanted to \"fix\" this problem back in Week 2, we'd have had two options: \n",
    "\n",
    " 1. add more data to our corpus and rebuild the `ARPA`-style language model\n",
    " 2. manually adjust the probability and backoff values in the existing `ARPA`-style language model\n",
    " \n",
    "**Note:** Doing #2 would **no longer** guarantee a probability distribution across the entire language model (*i.e.* all of the probabilities would **no longer** add up to `1.0`), but this isn't really a huge deal, especially if we were in a position where adding more data to the corpus wasn't practical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But even if we did #2, we'd still have to then rebuild the `FST` representing that language model.  So instead, let's modify the `FST` directly.  \n",
    "\n",
    "Presumably there are one of three things we'd want to do to an existing `FST` representation of a language model:\n",
    " 1. add a new word to the `FST`\n",
    " 2. remove a word from the `FST`\n",
    " 3. modify the \"likelihood\" of sequences containing a particular word in the `FST`\n",
    "   \n",
    "A few notes about these:\n",
    "\n",
    " 1. We actually can't add a new word to the `FST` that easily.  This is because the `FST` is built from an existing vocabulary of possible words.  To add a word to the `FST`, we'd have to first rebuild that vocabulary file, then add the new word.  Then modify the `FST`. \n",
    " 2. This is actually relatively easy to do.  Since each word has its own `node` in the `FST`, if we remove that `node`, along with all `arc`s to and from that `node`, we'd have removed the word from the language model.  **Note**:  Our language model has **both** `<unk>` modeled **and** `backoff` weights calculated.  So even an unseeen word or `n-gram` **can** get calculated by our `FST`.  So when we \"remove\" a word, all we can do is remove the explicit modeling of that word in particular `n-gram`s. \n",
    " 3. We can do this in one of two ways (or a combination of both ways):\n",
    "   - simply **increase**/**decrease** the likelihood of an existing `n-gram` containing a particular word\n",
    "     - *e.g.* if we wanted to increase the likelihood of `\"cat food\"` but **not** `\"cat ate\"`\n",
    "     - in this case, we'd simply need to **increase** the weight of the **one** `arc` from the `cat` `node` to the `food` `node`\n",
    "   - **increase**/**decrease** the likelihood of a particular word in **all** settings\n",
    "     - *e.g.* if we wanted to increase the likelihood of `cat` in **all** cases\n",
    "     - this would require updating **all** `arc`s into the `cat` `node`\n",
    "\n",
    "The less scientific part about cases #1 and #3, though, is how to determine what `weight` to set for the `arc`s.  And unless we are incredibly careful, we will not longer guarantee a probability distribution across our `FST`.  And while this, in and of itself, won't break our ASR pipeline, it may have unforseen consequences when modeling other sequences.\n",
    " \n",
    "In fact, let this be a warning: any of the above will have impacts throughout the language model, some of which may be unforseen and undesirable.  You've been warned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's see if we can update our `2-gram` `FST` so that `\"ate the mouse\"` ends up being approximately **three times more likely** than `\"ate the lion\"`.  We'll do this by updating the `weight`s of existing `arc`s (#3 above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### updating `weight`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fstmanipulate.py` has a method called `update_weight()` that will update **one** arc's weight at a time.  It will make the changes **in-place**.  Meaning, if you want to keep the original around, you need to make a copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat utils/fst_manipulate/fst_manipulate.py | grep -A7 \"update_weight(\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if we wanted to update the weight of the `arc` from `\"tyrannosaurus\"` to `\"rex\"`, we could do that like this...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the original\n",
    "fst_copy = fst_in.copy()\n",
    "# update the weights of the copy\n",
    "fstman.update_weight(\n",
    "    fst_in=fst_copy, \n",
    "    from_word=\"tyrannosaurus\", \n",
    "    to_word=\"rex\", \n",
    "    new_weight=99.999\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice two things:\n",
    "  1. The printed output says that **two** `arc`s were added.  That's because we were required to **remove** all `arc`s from a particular `state`, then add back the ones we wanted to keep along with the updated one.\n",
    "  2. If you look at the `arc` from `12` -> `13`, you'll now see the weight of `99.999` for `\"rex\"`.\n",
    "  \n",
    "**Note:** Since we made a **copy** of the `FST` before running `update_weights()`, we can still see the \"original\" `FST` (without the `weight` of `99.999`) captured by the variable `fst_in`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adding/deleting `arc`s\n",
    "\n",
    "Even though we won't be using them here, `fst_manipulate.py` has methods for `adding` and `removing` `arc`s as well.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fstman.add_arc()` will add an `arc` to the `FST` **provided** the `arc` to add is for a word **already** in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the original\n",
    "fst_add_arc = fst_in.copy()\n",
    "# update the weights of the copy\n",
    "fstman.add_arc(\n",
    "    fst_in=fst_add_arc, \n",
    "    from_word=\"tyrannosaurus\", \n",
    "    to_word=\"cat\", \n",
    "    weight=99.999\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the new `arc` from `node 12` to `node 8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_add_arc_dict, _ = fstman.index_fst(fst_add_arc)\n",
    "fst_add_arc_dict[\"tyrannosaurus\"][\"weights_to\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fstman.remove_arc()` will remove an `arc` from the `FST`.  Let's remove the `arc` we just added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to make a copy since we want to remove an arc from an existing FST\n",
    "fstman.remove_arc(\n",
    "    fst_in=fst_add_arc,\n",
    "    from_word=\"tyrannosaurus\",\n",
    "    to_word=\"cat\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_removed_arc_dict, _ = fstman.index_fst(fst_add_arc)\n",
    "fst_removed_arc_dict[\"tyrannosaurus\"][\"weights_to\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now back to solving our `\"ate the {lion,mouse}\"` problem..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `\"ate the {lion,mouse}\"`: updating `weight`s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the two `FST`s for the two sequences, we can see an obvious difference between the two:\n",
    " - `5` -> `7`: `\"lion\"/1.8627` v. `\"mouse\"/2.0433`\n",
    " \n",
    "**Note:** You may also notice the difference in `weight` of the final `arc` to `\"</s>\"`.  This is a consequence of the extra steps `kaldi` took in building this `FST`.  We'll ignore it for now and hope it doesn't have a large impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ate_the_lion_fst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ate_the_mouse_fst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, these `weight`s are $-ln$, so a **smaller** number is actually a **higher** likelihood.  Use `convert_neg_log_e()` to see them as probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lion_prob = fstman.convert_neg_log_e(\n",
    "    neg_log_e=1.8627\n",
    ")\n",
    "lion_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_prob = fstman.convert_neg_log_e(\n",
    "    neg_log_e=2.0433\n",
    ")\n",
    "mouse_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matches our intuitions because we know that `\"lion\"` appeared in our corpus **one more time** than `\"mouse\"`.  So a simple solution would be to **increase** the `weight` of the `\"mouse\"` arc so that it is equal to the `\"lion\"` `weight`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But remember there are multiple `\"mouse\"` arcs in our `FST`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_dict[\"mouse\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **two** `arc`s with `\"mouse\"` on the label:\n",
    "  1. from the `backoff` (`\"<eps>\"`) node\n",
    "  2. from the `\"the\"` node\n",
    "  \n",
    "So do we only update one?  Or both?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An argument could easily be made for updating **both** `arc`s, but since we really are only trying to handle a **specific** sequence (`\"ate the {lion,mouse}\"`), we can probably get away with only updating the `arc` from `\"the\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So first we'll make a copy of `fst_in` for our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_experiment_update_weight = fst_in.copy()\n",
    "fst_experiment_update_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's update the weight for the `arc` from `\"the\"` to `\"mouse\"` (in the visualization above, it's the `arc` labeled `\"mouse\"` from `node 4` to `node 5`) so that it equals the `weight` for the `arc` from `\"the\"` to `\"lion\"` (`arc` labeled `\"lion\"` from `node 4` to `node 11`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_lion_weight = fst_dict[\"the\"][\"weights_to\"][\"lion\"]\n",
    "the_lion_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fstman.update_weight(\n",
    "    fst_in=fst_experiment_update_weight,\n",
    "    from_word=\"the\",\n",
    "    to_word=\"mouse\",\n",
    "    new_weight=the_lion_weight\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm it worked.  We'll make an easy-to-use `<dict>` of our new `FST` with `index_fst()`. \n",
    "\n",
    "**Note:** Remember, that returns **two `<dict>s`**, the second of which we don't need right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_experiment_update_weight_dict, _ = fstman.index_fst(fst_experiment_update_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's make sure that the **only** thing that changed was the `weight` from `\"the\"` to `\"mouse\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_dict[\"the\"][\"weights_to\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_experiment_update_weight_dict[\"the\"][\"weights_to\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked!  Now let's see if the likelihood of the sequences is fixed.  Below are the values for the sequences `\"ate the mouse\"` and `\"ate the lion\"` from the original `FST`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ate_the_mouse_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ate_the_lion_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't do anything to change the cost of `\"ate the lion\"`, so let's determine the cost of `\"ate the mouse\"` with our updated `FST`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates the composed-FST of our sequence and the full FST\n",
    "ate_the_mouse_updated_fst = fstman.check_sequence(  \n",
    "    seq_string=ate_the_mouse,    \n",
    "    lm_fst=fst_experiment_update_weight                  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the cost and convert to probability\n",
    "ate_the_mouse_updated_cost = fstman.convert_neg_log_e(\n",
    "    neg_log_e=fstman.calculate_cost(\n",
    "        fst_in=ate_the_mouse_updated_fst\n",
    "    )\n",
    ")\n",
    "ate_the_mouse_updated_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well we managed to **increase** the likelihood of the sequence to be **more** than that of `\"ate the lion\"`, but only barely.  What went wrong?  Let's have a look at the `composed` `FST` and compare it to the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ate_the_mouse_updated_fst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ate_the_mouse_fst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We managed to **increase** the `weight` of `\"mouse\"`, but it still didn't do what we had hoped.  Let's look at the `composed` `FST` for `\"ate the lion\"` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ate_the_lion_fst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem actually lies in the `arc` `weight` for `\"ate\"`.  Right now that `weight` is the **same** for both sequences, but that's not really what we want!  \n",
    "\n",
    "Intuitively, we want it to be that \"eating a mouse\" is **more likely** than \"eating a lion\".  But in the `FST`s above, that's not happening.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't have this problem with `\"lion ate\"` and `\"mouse ate\"`.  See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lion_ate_fst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_ate_fst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the `\"ate\"` that follows `\"mouse\"` is much smaller than the `\"ate\"` that follows `\"lion\"`.  But that's because we were able to model the `2-gram` `\"{lion,mouse} ate\"` **explicitly** in our `2-gram` model.  But in the case we're trying to solve now, it's a `3-gram` `\"ate the {lion,mouse}\"`, and that darn `\"the\"` is messing everything up!\n",
    "\n",
    "Because if we look at all the `arc`s leaving the `node` for `\"ate\"` in our `FST`, we see only two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_dict[\"ate\"][\"weights_to\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That shouldn't be surprising becuase if we look at our corpus again, we'll see that the **only** `2-gram` that we explicitly modeled with `\"ate\"` is `\"ate the\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat resource_files/language_model/animal_corpus.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if tried to change the `weight` for the `arc` from `\"ate\"` to the `\"the\"`, it would affect `\"ate the lion\"` and `\"ate the mouse\"` equally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And like we discovered in `2.2 Examining language models`, the only way to guarantee we get the outcome we want in this case is to model the `3-gram`s explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## converting a `3-gram` language model to `FST`\n",
    "\n",
    "We can follow the same process to convert our `3-gram` language model into an `FST`, but the complexity increases dramatically.  `resource_files/fst` has an `FST` built from our `3-gram` language model that we can look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fst_3_gram = openfst.Fst.read(\"resource_files/fst/animal_fst-3_gram.fst\")\n",
    "fst_3_gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that even though it has the same small vocabulary size as our `2-gram` language model, it's much larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states_2_gram = len(list(fst_in.states()))\n",
    "num_states_3_gram = len(list(fst_3_gram.states()))\n",
    "print(\"number of states in 2-gram model: {}\\nnumber of states in 3-gram model: {}\".format(\n",
    "    num_states_2_gram, num_states_3_gram\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
