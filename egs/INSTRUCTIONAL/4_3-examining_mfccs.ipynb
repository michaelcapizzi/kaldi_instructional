{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: This `notebook` is built for a `python` kernel, and so the default setting for each cell is `python`.  But you can use `%%bash` at the beginning of any cell to utilize `shell`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.feature_viz.feature_viz as fv\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3: Examining the `MFCC`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `INSTRUCTIONAL` directory has a copy of [this third-party repository](https://github.com/vesis84/kaldi-io-for-python) in `utils/feature_viz`.  `utils/feature_viz/kaldi_io.py` has methods that allow us to read from (and write to...though we won't be using that) `.ark` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls utils/feature_viz/kaldi_io_for_python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there is a `python` module called `feature_viz.py` that has some methods wrapping `kaldi_io.py` that we'll use below to examine the `MFCC` features we extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing an audio sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of our `mfcc` features are located in the `mfcc` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls mfcc | grep mfcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice a number of files equal to the parallelization that we utilized during feature extraction, and for each `int`, there will be an `.ark` and a `.scp` file.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If there is a *particular* audio sample that you'd like to inspect, you'll have to find which `.ark` file it resides in.\n",
    "\n",
    "For this example, we'll look at an example with some interesting sounds in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head raw_data/librispeech-transcripts.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's find which `.ark` it is in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "for f in `ls mfcc/raw_mfcc_*.scp`; do\n",
    "    match=$(cat $f | grep \"1272-128104-0000\")\n",
    "    if [[ -z $match ]]; then\n",
    "        echo \"not found in $f\"\n",
    "    else\n",
    "        echo \"found in $f: $match\"\n",
    "    fi\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE = \"1272-128104-0000\"\n",
    "ARK_SAMPLE = \"raw_mfcc_train_dir.1.scp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`read_in_features()` will read in the features for *all* of the utterances found in our `.ark`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_in = fv.read_in_features(\"mfcc/{}\".format(ARK_SAMPLE))\n",
    "list(feats_in.keys())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look more closely at our sample, the `value` is a `numpy array` representing the features for each frame in the utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_in[SAMPLE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the feature shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of this `array` is `(num_frames x num_features)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_in[SAMPLE].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two functions in `feature_viz.py` that can also easily extract this information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fv.get_num_frames(feats_in[SAMPLE]), fv.get_num_features(feats_in[SAMPLE])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look back at the `MFCC` configuration file we used to extract these features, we'll see where this shape comes from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat conf/mfcc_defaults.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`--num-ceps` dictates the number of columns our features will have.\n",
    "\n",
    "And the number of frames depends on `--frame-length` and `--frame-shift` (along with the actual length of the audio of course).  In this case, each `frame` represents `25 ms` of audio, and the \"next frame\" is shifted `10 ms` to the right, and then consists of the next `25 ms`.  So our functions are overlapped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since \"1272-128104-0003\" is a significantly longer transcript, we can assume that it will have more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat raw_data/librispeech-transcripts.txt | grep -E \"1272-128104-000[03]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fv.get_num_frames(feats_in[\"1272-128104-0003\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, it will have the same number of `features`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fv.get_num_features(feats_in[\"1272-128104-0003\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`feature_viz.py` has a method `plot_frames()` that will plot the `MFCC`s for `n` consecutive frames of an audio sample\n",
    "\n",
    "**Note:** In order to view the plot directly in this notebook, you need to run `py.iplot([output])` where `output` is the returned value from `plot_frames()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the one *required* argument is a numpy array of features for any number of frames\n",
    "py.iplot(\n",
    "    fv.plot_frames(\n",
    "        frames=feats_in[SAMPLE][60:61]   # [x:y] will return the x^th frame\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see the first frame of features for our audio sample.  If you prefer, you can view these as a `bar` graph with an added argument `mode=bar` to the function.  This *may* be more intuitive as the `vector` is `discrete` (*e.g.* there is **no value** for `x=3.5`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.iplot(\n",
    "    fv.plot_frames(\n",
    "        frames=feats_in[SAMPLE][60:61],   # [x:y] will return the x^th frame\n",
    "        mode='bar'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`plot_frames()` can also plot multiple **consecutive** frames on the same graph.  Below are five consecutive frames of audio.\n",
    "\n",
    "**Note:** `plotly` allows you to click \"on\" and \"off\" any particular line by clicking on them in the legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.iplot(\n",
    "    fv.plot_frames(\n",
    "        frames=feats_in[SAMPLE][60:68]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can, again, view these as a `bar` graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.iplot(\n",
    "    fv.plot_frames(\n",
    "        frames=feats_in[SAMPLE][60:68],\n",
    "        mode='bar'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, it *may* be easier to compare a *particular coefficient* (*e.g.* `x` value) across frames.  For example, you can see here that `x=1` is non-negative for all frames in this sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including `phone` information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If provided with the information, `plot_frames()` can also label each `frame` by its *predicted* phone.  It is a *predicted* phone because the **alignment*** of frames to a transcript is an integral part of the ASR pipeline, but its accuracy is dependent on the quality of the pipeline.\n",
    "\n",
    "In our case, we have **not yet** done the steps in the pipeline that generate these `alignment` files (typically called `ali.*.gz`).   But for the sake of these visualizations, the alignments we need are included in `resource_files/feature_viz/all_ali`.  This is an `un-compressed` (`gzip -d`), concatenated (of all of the parallelized outputs) file that contains alignmenet information for all of the audio in our training subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls resource_files/feature_viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a `C++` function called `ali-to-phones` to inspect these alignments. \n",
    "\n",
    "**Note:** Normally we could `source` `path.sh` (`. path.sh`) in order to avoid providing full paths to these C++ functions.  But because we are in a `python` `kernel` and only using `bash` in individual cells, that doesn't work.  So we have to provide full paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "${KALDI_PATH}/src/bin/ali-to-phones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use this function, we also need an acoustic model (again, something we haven't built yet in our pipeline).  A model is provided in `resource_files/feature_viz/model_for_alignments.mdl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# using ark,t:- we are telling the function to output to STDOUT\n",
    "${KALDI_PATH}/src/bin/ali-to-phones \\\n",
    "    resource_files/feature_viz/model_for_alignments.mdl \\\n",
    "    ark:resource_files/feature_viz/all_ali \\\n",
    "    ark,t:- | grep \"1272-128104-0000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates a sequence of indexes representing the sequence of phones for the given utterance.  If we want to see the actual phones these indexes refer to, we need to `pipe` this output to `int2sym.pl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "${KALDI_INSTRUCTIONAL_PATH}/utils/int2sym.pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that this method requires a `symtab` (symbol table), which we have already built in `data/lang/phones.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head data/lang/phones.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat raw_data/librispeech-transcripts.txt | grep -E \"1272-128104-0000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "${KALDI_PATH}/src/bin/ali-to-phones \\\n",
    "    resource_files/feature_viz/model_for_alignments.mdl \\\n",
    "    ark:resource_files/feature_viz/all_ali \\\n",
    "    ark,t:- |\\\n",
    "    ${KALDI_INSTRUCTIONAL_PATH}/utils/int2sym.pl -f 2- data/lang/phones.txt |\\\n",
    "    grep \"1272-128104-0000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But you'll notice that this is waaaaay less sounds than the 584 that we know exist for this audio sample.  In this case, the function collapsed a consecutive sequence of the same phone for easy \"viewing\".  This shouldn't be surprising that a particular sound might last for longer than one frame (which is `25 ms` in our case).\n",
    "\n",
    "We can add the `--per-frame` argument to get a `phone` for each `frame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "${KALDI_PATH}/src/bin/ali-to-phones \\\n",
    "    --per-frame=true \\\n",
    "    resource_files/feature_viz/model_for_alignments.mdl \\\n",
    "    ark:resource_files/feature_viz/all_ali \\\n",
    "    ark,t:- |\\\n",
    "    ${KALDI_INSTRUCTIONAL_PATH}/utils/int2sym.pl -f 2- data/lang/phones.txt |\\\n",
    "    grep \"1272-128104-0000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so now we have a full command we can run that will generate this output for each audio sample in our training set.  We will run it below and save it to `resource_files/feature_viz/all_ali_phoned`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "${KALDI_PATH}/src/bin/ali-to-phones \\\n",
    "    --per-frame=true \\\n",
    "    resource_files/feature_viz/model_for_alignments.mdl \\\n",
    "    ark:resource_files/feature_viz/all_ali \\\n",
    "    ark,t:- |\\\n",
    "    ${KALDI_INSTRUCTIONAL_PATH}/utils/int2sym.pl -f 2- data/lang/phones.txt > resource_files/feature_viz/all_ali_phoned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat resource_files/feature_viz/all_ali_phoned | grep \"1272-128104-0000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`feature_viz.py` also has a method for reading in these alignments into a `<dict>`.  And the `value` will be a `<list>` of phones equal to the number of frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ali_in = fv.read_in_alignments(\"resource_files/feature_viz/all_ali_phoned\")\n",
    "ali_in[SAMPLE][60:68]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ali_in[SAMPLE]) == fv.get_num_frames(feats_in[SAMPLE])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can provide the relevant `<list>` of phones to `plot_frames()` to label each frame with its *predicted* phone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.iplot(\n",
    "    fv.plot_frames(\n",
    "        frames=feats_in[SAMPLE][60:68],\n",
    "        phones=ali_in[SAMPLE][60:68]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that these 8 frames *most likely* represent *three* different phones.  \n",
    "\n",
    "Again, clicking on individual lines in the legend will turn that line \"on\"/\"off\" from the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including \"average\" `mfcc` information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`feature_viz.py` also has a function that will calculate the *average* `MFCC` vector for each `phone` in our training subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to build a `<dict>` that groups all of the `MFCC` vectors for each phone together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dict = fv.get_grouped_phones_dict(\n",
    "                    feats_dict=feats_in, \n",
    "                    ali_dict=ali_in\n",
    ")\n",
    "list(grouped_dict.keys())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `value` is a `<list>` of all the examples of that phone in our subset.  In this case, there are 5955 *predicteed* `F_B` phones in our subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(grouped_dict[\"F_B\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then run `get_average_mfccs()` to generate the `mean` `MFCC` vector for each phone.  Below is the `mean` `MFCC` vector for all of the `F_B`s seen in our subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ave_dict = fv.get_average_mfccs(\n",
    "    phones_dict=grouped_dict\n",
    ")\n",
    "ave_dict[\"F_B\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`plot_frames()` can also show you the average vector of each frame that appears in your plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.iplot(\n",
    "    fv.plot_frames(\n",
    "        frames=feats_in[SAMPLE][60:68],\n",
    "        phones=ali_in[SAMPLE][60:68],\n",
    "        average_mfccs_dict=ave_dict\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now compare the particular `MFCC` vectors for a given phone to its average vector.  This *may* be more useful when plotted over the `bar graph` version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.iplot(\n",
    "    fv.plot_frames(\n",
    "        frames=feats_in[SAMPLE][60:68],\n",
    "        phones=ali_in[SAMPLE][60:68],\n",
    "        average_mfccs_dict=ave_dict,\n",
    "        mode='bar'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study of `IH`\n",
    "\n",
    "Let's look more closely at a particular `phone`.  There are *five* instances of `IH` in our chosen sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat raw_data/librispeech-transcripts.txt | grep -E \"1272-128104-0000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the frames that correspond to them.\n",
    "\n",
    "**Note:** The phone is `IH1` which indicates the \"second\" pronunciation of `IH` in our phones set.  There is also a `IH0` and a `IH2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat data/lang/phones.txt | grep IH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enumerated_phones = list(enumerate(ali_in[SAMPLE]))\n",
    "\n",
    "list(filter(\n",
    "    lambda x: \"IH\" in x[1],\n",
    "    enumerated_phones)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see that those *five* instances correspond to the following frames:\n",
    "    - 59-62\n",
    "    - 96-98\n",
    "    - 125-131\n",
    "    - 243-245\n",
    "    - 470-473"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_1 = {\n",
    "    \"start\": 59,\n",
    "    \"stop\": 63    # +1 to be inclusive\n",
    "}\n",
    "\n",
    "ex_2 = {\n",
    "    \"start\": 96,\n",
    "    \"stop\": 99    # +1 to be inclusive\n",
    "}\n",
    "\n",
    "ex_3 = {\n",
    "    \"start\": 125,\n",
    "    \"stop\": 132    # +1 to be inclusive\n",
    "}\n",
    "\n",
    "ex_4 = {\n",
    "    \"start\": 243,\n",
    "    \"stop\": 246    # +1 to be inclusive\n",
    "}\n",
    "\n",
    "ex_5 = {\n",
    "    \"start\": 470,\n",
    "    \"stop\": 474    # +1 to be inclusive\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ali_in[SAMPLE][ex_1[\"start\"]:ex_1[\"stop\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot each sequence of phones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ex_1`: `IH` in \"MISTER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.iplot(\n",
    "    fv.plot_frames(\n",
    "        frames=feats_in[SAMPLE][ex_1[\"start\"]:ex_1[\"stop\"]],\n",
    "        phones=ali_in[SAMPLE][ex_1[\"start\"]:ex_1[\"stop\"]],\n",
    "        average_mfccs_dict=ave_dict\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.iplot(\n",
    "    fv.plot_frames(\n",
    "        frames=feats_in[SAMPLE][ex_1[\"start\"]:ex_1[\"stop\"]],\n",
    "        phones=ali_in[SAMPLE][ex_1[\"start\"]:ex_1[\"stop\"]],\n",
    "        average_mfccs_dict=ave_dict,\n",
    "        mode='bar'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ex_2`: `IH` in \"QUILTER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.iplot(\n",
    "    fv.plot_frames(\n",
    "        frames=feats_in[SAMPLE][ex_2[\"start\"]:ex_2[\"stop\"]],\n",
    "        phones=ali_in[SAMPLE][ex_2[\"start\"]:ex_2[\"stop\"]],\n",
    "        average_mfccs_dict=ave_dict\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.iplot(\n",
    "    fv.plot_frames(\n",
    "        frames=feats_in[SAMPLE][ex_2[\"start\"]:ex_2[\"stop\"]],\n",
    "        phones=ali_in[SAMPLE][ex_2[\"start\"]:ex_2[\"stop\"]],\n",
    "        average_mfccs_dict=ave_dict,\n",
    "        mode='bar'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ex_3`: `IH` in \"IS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.iplot(\n",
    "    fv.plot_frames(\n",
    "        frames=feats_in[SAMPLE][ex_3[\"start\"]:ex_3[\"stop\"]],\n",
    "        phones=ali_in[SAMPLE][ex_3[\"start\"]:ex_3[\"stop\"]],\n",
    "        average_mfccs_dict=ave_dict\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.iplot(\n",
    "    fv.plot_frames(\n",
    "        frames=feats_in[SAMPLE][ex_3[\"start\"]:ex_3[\"stop\"]],\n",
    "        phones=ali_in[SAMPLE][ex_3[\"start\"]:ex_3[\"stop\"]],\n",
    "        average_mfccs_dict=ave_dict,\n",
    "        mode='bar'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ex_4`: `IH` in \"MIDDLE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.iplot(\n",
    "    fv.plot_frames(\n",
    "        frames=feats_in[SAMPLE][ex_4[\"start\"]:ex_4[\"stop\"]],\n",
    "        phones=ali_in[SAMPLE][ex_4[\"start\"]:ex_4[\"stop\"]],\n",
    "        average_mfccs_dict=ave_dict\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.iplot(\n",
    "    fv.plot_frames(\n",
    "        frames=feats_in[SAMPLE][ex_4[\"start\"]:ex_4[\"stop\"]],\n",
    "        phones=ali_in[SAMPLE][ex_4[\"start\"]:ex_4[\"stop\"]],\n",
    "        average_mfccs_dict=ave_dict,\n",
    "        mode='bar'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ex_5`: `IH` in \"HIS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.iplot(\n",
    "    fv.plot_frames(\n",
    "        frames=feats_in[SAMPLE][ex_5[\"start\"]:ex_5[\"stop\"]],\n",
    "        phones=ali_in[SAMPLE][ex_5[\"start\"]:ex_5[\"stop\"]],\n",
    "        average_mfccs_dict=ave_dict\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.iplot(\n",
    "    fv.plot_frames(\n",
    "        frames=feats_in[SAMPLE][ex_4[\"start\"]:ex_4[\"stop\"]],\n",
    "        phones=ali_in[SAMPLE][ex_4[\"start\"]:ex_4[\"stop\"]],\n",
    "        average_mfccs_dict=ave_dict,\n",
    "        mode='bar'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ex_3` and `ex_5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `ex_3` and `ex_5` are \"IS\" and \"HIS\", respectively, let's plot them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_3_5_frames = np.vstack(\n",
    "    (\n",
    "        feats_in[SAMPLE][ex_3[\"start\"]:ex_3[\"stop\"]],\n",
    "        feats_in[SAMPLE][ex_5[\"start\"]:ex_5[\"stop\"]]\n",
    "    )\n",
    ")\n",
    "ex_3_5_frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_3_5_phones = ali_in[SAMPLE][ex_3[\"start\"]:ex_3[\"stop\"]] + ali_in[SAMPLE][ex_5[\"start\"]:ex_5[\"stop\"]]\n",
    "ex_3_5_phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.iplot(\n",
    "    fv.plot_frames(\n",
    "        frames=ex_3_5_frames,\n",
    "        phones=ex_3_5_phones,\n",
    "        average_mfccs_dict=ave_dict,\n",
    "        mode='bar'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's hard to see any similarity in the individual frames of each `IH`, but we *can* see that the average phone for `IH_B` (when `IH` comes at the **beginning** of a word) and the average phone for `IH_I` (when `IH` comes in the **middle** of a word) are quite similar, which shouldn't be too surprising."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But then the question remains why these individual frames of a particular `IH` differ so much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### phones in context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps it's because each of these sequence of `IH` phones comes in a different \"context\" (in terms of which phone came **before** and **after** them).  And this has a direct effect on the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `ex_1` (\"MIS..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-plot `ex_1` (\"MISTER\"), but this time with a few frames before and after added for context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.iplot(\n",
    "    fv.plot_frames(\n",
    "        frames=feats_in[SAMPLE][ex_1[\"start\"]-2:ex_1[\"stop\"]+2],\n",
    "        phones=ali_in[SAMPLE][ex_1[\"start\"]-2:ex_1[\"stop\"]+2],\n",
    "        average_mfccs_dict=ave_dict\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare frame `3/8` with `4/8` (the first two frames of `IH`).  They are very similar.\n",
    "\n",
    "Now compare `3/8` to `6/8` (the *first* and *last* frames of `IH`).  Very different.\n",
    "\n",
    "Now compare `2/8` to `3/8` (the *last* `M` frame with the first `IH` frame).  And compare `7/9` with `8/9` (the *last* `IH` frame and the *first* `S` frame)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `ex_2` (\"...WIL...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this sequence (`ex_1`) with `ex_2` (\"QUILTER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.iplot(\n",
    "    fv.plot_frames(\n",
    "        frames=feats_in[SAMPLE][ex_2[\"start\"]-2:ex_2[\"stop\"]+2],\n",
    "        phones=ali_in[SAMPLE][ex_2[\"start\"]-2:ex_2[\"stop\"]+2],\n",
    "        average_mfccs_dict=ave_dict\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how much \"tighter\" all these frames line up with each other.  In other words, the impact of the `W` and the `L` on the `IH` frames is much less noticeable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `ex_2` (\"...R IS\")\n",
    "\n",
    "This example shows the **last two** frames of the **previous** word, \"QUILTER\" along with the majority of the frames associated with \"IS\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.iplot(\n",
    "    fv.plot_frames(\n",
    "        frames=feats_in[SAMPLE][ex_3[\"start\"]-2:ex_3[\"stop\"]+2],\n",
    "        phones=ali_in[SAMPLE][ex_3[\"start\"]-2:ex_3[\"stop\"]+2]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with `frame 1/11` as the *only* visible line, add each next frame one at a time.  Notice how at first, the `IH1_B` frames (`3/11` and `4/11`) are almost *exactly* the same as the `ER0_E` frames.  But over time, they begin to noticeable shift -- one could argue -- *towards* the `Z_E`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
